{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "fashion_mnist_train = FashionMNIST(\"./data/FashionMNIST\", train=True,\n",
    "                                   download=True, transform=transforms.ToTensor())\n",
    "fashion_mnist_test = FashionMNIST(\"./data/FashionMNIST\", train=False,\n",
    "                                   download=True, transform=transforms.ToTensor())\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(fashion_mnist_train, batch_size=batch_size,\n",
    "                          shuffle=True, num_workers=14)\n",
    "test_loader = DataLoader(fashion_mnist_test, batch_size=batch_size,\n",
    "                         shuffle=True, num_workers=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenLayer(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.flatten(x, start_dim=1)\n",
    "\n",
    "conv_net = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.Dropout2d(0.25),\n",
    "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.Dropout2d(0.25),\n",
    "    FlattenLayer()\n",
    ")\n",
    "\n",
    "test_input = torch.ones(1, 1, 28, 28)\n",
    "conv_output_size = conv_net(test_input).size()[-1]\n",
    "\n",
    "mlp = nn.Sequential(\n",
    "    nn.Linear(conv_output_size, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(200),\n",
    "    nn.Dropout(0.25),\n",
    "    nn.Linear(200, 10)\n",
    ")\n",
    "\n",
    "net = nn.Sequential(\n",
    "    conv_net,\n",
    "    mlp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_net(net, data_loader, device=\"cpu\"):\n",
    "    net.eval()\n",
    "    ys = []\n",
    "    ypreds = []\n",
    "    for x, y in data_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        with torch.no_grad():\n",
    "            _, y_pred = net(x).max(1)\n",
    "        ys.append(y)\n",
    "        ypreds.append(y_pred)\n",
    "    ys = torch.cat(ys)\n",
    "    ypreds = torch.cat(ypreds)\n",
    "    acc = (ys == ypreds).float().sum() / len(ys)\n",
    "    return acc.item()\n",
    "\n",
    "train_losses = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "\n",
    "def train_net(net, train_loader, test_loader, optimizer_cls=optim.Adam,\n",
    "              loss_fn=nn.CrossEntropyLoss(), n_iter=10, device=\"cpu\", writer=None):\n",
    "    optimizer = optimizer_cls(net.parameters())\n",
    "    for epoch in range(n_iter):\n",
    "        running_loss = 0.0\n",
    "        net.train()\n",
    "        n = 0\n",
    "        n_acc = 0\n",
    "        for i, (xx, yy) in tqdm.tqdm(enumerate(train_loader),\n",
    "                                     total = len(train_loader)):\n",
    "            xx = xx.to(device)\n",
    "            yy = yy.to(device)\n",
    "            h = net(xx)\n",
    "            loss = loss_fn(h, yy)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            n += len(xx)\n",
    "            y_pred = h.argmax(1)\n",
    "            n_acc += (y_pred == yy).float().sum().item()\n",
    "        train_losses.append(running_loss / i)\n",
    "        train_acc.append(n_acc / n)\n",
    "        val_acc.append(eval_net(net, test_loader, device))\n",
    "        print(\"epoch: {}\\ttrain_loss: {:.3f}\\ttrain_acc: {:.3f}\\tval_acc: {:.3f}\".format(\n",
    "             epoch, train_losses[-1], train_acc[-1], val_acc[-1]), flush=True)\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\"train_loss\", train_losses[-1], epoch)\n",
    "            writer.add_scalars(\"accuracy\", {\"train\":train_acc[-1],\n",
    "                                            \"validation\":val_acc[-1]}, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:01<00:00, 265.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\ttrain_loss: 0.321\ttrain_acc: 0.883\tval_acc: 0.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:01<00:00, 266.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\ttrain_loss: 0.281\ttrain_acc: 0.896\tval_acc: 0.900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:01<00:00, 262.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\ttrain_loss: 0.260\ttrain_acc: 0.904\tval_acc: 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:01<00:00, 260.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3\ttrain_loss: 0.241\ttrain_acc: 0.911\tval_acc: 0.908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:01<00:00, 263.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4\ttrain_loss: 0.232\ttrain_acc: 0.914\tval_acc: 0.906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:01<00:00, 273.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\ttrain_loss: 0.221\ttrain_acc: 0.918\tval_acc: 0.910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:01<00:00, 278.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6\ttrain_loss: 0.211\ttrain_acc: 0.921\tval_acc: 0.914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:01<00:00, 266.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7\ttrain_loss: 0.202\ttrain_acc: 0.923\tval_acc: 0.916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:01<00:00, 271.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8\ttrain_loss: 0.196\ttrain_acc: 0.927\tval_acc: 0.918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:01<00:00, 269.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9\ttrain_loss: 0.188\ttrain_acc: 0.929\tval_acc: 0.916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:01<00:00, 273.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10\ttrain_loss: 0.183\ttrain_acc: 0.932\tval_acc: 0.917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:01<00:00, 235.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11\ttrain_loss: 0.179\ttrain_acc: 0.933\tval_acc: 0.916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:01<00:00, 262.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12\ttrain_loss: 0.173\ttrain_acc: 0.935\tval_acc: 0.921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:01<00:00, 276.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13\ttrain_loss: 0.168\ttrain_acc: 0.937\tval_acc: 0.919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:01<00:00, 273.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14\ttrain_loss: 0.166\ttrain_acc: 0.937\tval_acc: 0.921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:01<00:00, 267.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15\ttrain_loss: 0.161\ttrain_acc: 0.939\tval_acc: 0.920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:01<00:00, 274.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16\ttrain_loss: 0.153\ttrain_acc: 0.943\tval_acc: 0.919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:01<00:00, 268.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17\ttrain_loss: 0.153\ttrain_acc: 0.942\tval_acc: 0.918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:01<00:00, 260.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18\ttrain_loss: 0.151\ttrain_acc: 0.944\tval_acc: 0.919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:01<00:00, 265.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19\ttrain_loss: 0.149\ttrain_acc: 0.944\tval_acc: 0.921\n"
     ]
    }
   ],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(\"./tmp/cnn\")\n",
    "\n",
    "net.to(\"cuda:0\")\n",
    "train_net(net, train_loader, test_loader, n_iter=20,\n",
    "          device=\"cuda:0\", writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
